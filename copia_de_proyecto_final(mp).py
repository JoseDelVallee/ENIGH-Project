# -*- coding: utf-8 -*-
"""Copia de proyecto_final(MP).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pe-ntb9bSBmeEM67dxY_5K9VSjg6Renu

# Proyecto Modelado Predictivo


---


## Objetivo de Desarrollo Sostenible No.8 Trabajo Decente y Crecimiento Económico

---

## Integrantes:


*   Arias Morales Yahir
*   Ayala García Oscar Galo
*   Piña del Valle José
*   Rivera García Axel Maximiliano

## Librerias
"""

pip install streamlit

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import scipy.stats as stats
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import pairwise_tukeyhsd
warnings.filterwarnings("ignore")

pd.options.display.float_format = '{:.5f}'.format

"""## Importación de los datos"""

df = pd.read_csv("concentradohogarproyecto.csv")
df = df.drop(columns="Unnamed: 0")

df = df[df['ingtrab'] != 0]

"""## Entendimiento del problema"""

# Filas y columnas
print("Filas y columnas del DataFrame:")
print(df.shape)

#Variables a utilizar
print("Variables a utilizar:")
print(pd.DataFrame(df.columns))

#Distribución de nuestra variable objetivo, en este caso del ingreso por trabajo
print("Distribución de la variable objetivo (ingreso por trabajo):")
print(df["ingtrab"].describe())

df.info()

df.describe(include='all')

#Distribucion del ingreso por trabajo
plt.figure(figsize=(10, 6))
sns.histplot(df['ingtrab'], bins=30, kde=True)
plt.title('Distribución del Ingreso por Trabajo')
plt.xlabel('Ingreso por Trabajo')
plt.ylabel('Frecuencia')
plt.ticklabel_format(style='plain', axis='x')
plt.grid()
plt.show()

# Correlaciones entre variables numéricas
plt.figure(figsize=(12, 8))
correlation_matrix = df.corr()
print(correlation_matrix)

"""## Limipieza y preprocesamiento de datos

### Variables numericas
"""

df_numericas = df.select_dtypes(include=float)
df_numericas = df_numericas.drop(columns=["ingtrab"])

df_numericas

#Visualizacion de valores atipicos
plt.figure(figsize=(12, 8))
sns.boxplot(data=df_numericas)
plt.title('Boxplot de Variables Numéricas')
plt.xticks(rotation=45)
plt.ticklabel_format(style='plain', axis='y')
plt.grid()
plt.show()

"""### Variables categoricas"""

df_categoricas = df.select_dtypes(include=int)

#histograma de variables categoricas
plt.figure(figsize=(12, 8))
df_categoricas.hist(bins=30, figsize=(15, 10), layout=(3, 3), edgecolor='black')
plt.suptitle('Histogramas de Variables Categóricas')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

#Limpieza de outliers
def remove_outliers(df, threshold=3):
    for col in df:
        mean = df[col].mean()
        std_dev = df[col].std()
        outliers = (df[col] - mean).abs() > threshold * std_dev
        df = df[~outliers]
    return df

df_cleaned = remove_outliers(df.copy(), threshold=3)

#Visualizacion de las columnas sin outliers
plt.figure(figsize=(12, 8))
sns.boxplot(data=df_cleaned)
plt.title('Boxplot de Variables Numéricas sin Outliers')
plt.xticks(rotation=45)
plt.ticklabel_format(style='plain', axis='y')
plt.grid()
plt.show()

# Contar valores únicos por columna
unique_counts = df.nunique()
print("Valores únicos por columna:")
print(unique_counts)

"""## Transformación logaritmica de los datos"""

#Transformación de las varibles numericas
def transform_numerical(df):
    for col in df.select_dtypes(include=float).columns:
        if df[col].skew() > 1 or df[col].skew() < -1:
            df[col] = np.log1p(df[col])
    return df

#Transformacion Box-Cox
def transform_boxcox(df):
    for col in df.select_dtypes(include=float).columns:
        if df[col].skew() > 1 or df[col].skew() < -1:
            df[col], _ = stats.boxcox(df[col] + 1)
    return df

# df = transform_numerical(df)

df = transform_boxcox(df)

#Distribucion de todas las variables
plt.figure(figsize=(12, 6))
num_cols = len(df.columns)
num_rows = (num_cols + 2) // 3
for i, col in enumerate(df.columns):
    plt.subplot(num_rows, 3, i+1)
    sns.histplot(df[col], kde=True)
    plt.title(col)
plt.ticklabel_format(style='plain', axis='x')
plt.tight_layout()
plt.show()

"""## Analisis Exploratorio de Datos (EDA)

### Ingreso vs las demas variables

#### Variables numericas
"""

#Ingreso por trabajo vs otras variables numericas

df_numericas = df.select_dtypes(include=float)
df_numericas = df_numericas.drop(columns=["ingtrab"])
for col in df_numericas.columns:
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=df[col], y=df['ingtrab'])
    plt.title(f'Ingreso por Trabajo vs {col}')
    plt.xlabel(col)
    plt.ylabel('Ingreso por Trabajo')
    plt.ticklabel_format(style='plain', axis='x')
    plt.grid()
    plt.show()

"""Para el

### Variables categoricas
"""

# ingtrab vs variables categoricas
df_categoricas = df.select_dtypes(include=int)
for col in df_categoricas.columns:
    plt.figure(figsize=(10, 6))
    sns.barplot(x=df[col], y=df['ingtrab'])
    plt.title(f'Ingreso por Trabajo vs {col}')
    plt.xlabel(col)
    plt.ylabel('Ingreso por Trabajo')
    plt.ticklabel_format(style='plain', axis='y')
    plt.grid()
    plt.show()

"""## Visualización de los Q - Q plots"""

plt.figure(figsize=(10, 20))
for i, col in enumerate(df.columns):
    plt.subplot(num_cols, 3, i+1)
    stats.probplot(df[col], dist="norm", plot=plt)
    plt.title(col)
plt.tight_layout()

"""##### Dado que nuestro enfoque es saber la desigualdad, algunas graficas que podrian ser utilies"""

sns.boxplot(x='sexo_jefe', y='ingtrab', data=df)
df.groupby('sexo_jefe')['ingtrab'].describe().reset_index()

sns.boxplot(x='educa_jefe', y='ingtrab', data=df)
df.groupby('educa_jefe')['ingtrab'].describe().reset_index()

sns.boxplot(x='tam_loc', y='ingtrab', data=df)
df.groupby('tam_loc')['ingtrab'].describe().reset_index()

sns.boxplot(x='est_socio', y='ingtrab', data=df)
df.groupby('est_socio')['ingtrab'].describe().reset_index()

sns.catplot(x='educa_jefe', y='ingtrab', hue='sexo_jefe', kind='box', data=df)

"""## ANOVA"""

#Funcion que realiza el ANOVA de diferentes variables categoricas
def anova_categorical(df, target, categorical_vars):
    results = {}
    for var in categorical_vars:
        model = ols(f'{target} ~ C({var})', data=df).fit()
        anova_table = sm.stats.anova_lm(model, typ=2)
        results[var] = anova_table['PR(>F)'][0]
    return pd.DataFrame(results, index=['p-value']).T

anova_categorical_vars = ['sexo_jefe', 'educa_jefe', 'tam_loc', 'est_socio']
anova_results = anova_categorical(df, 'ingtrab', anova_categorical_vars)
print("Resultados del ANOVA para variables categóricas:")
print(anova_results)

"""En el calulo del ANOVA, dado que los p-value son < 0.05, significa que las variables categoricas son extremadamente significantes con respecto a la variable objetivo que es el ingreso por trabajo."""

def tukey_test(df, target, categorical_var):
    tukey = pairwise_tukeyhsd(endog=df[target], groups=df[categorical_var], alpha=0.05)
    return tukey

"""La prueba Tukey HSD (Honest Significant Difference) nos ayuda a encontrar diferencias significativas entre los diferentes grupos de la variables categoricas, con un 95% de confianza. Asi pues tomamos que el p-value sea < 0.05 para que la **hipotesis nula** (Existe diferencia significativa entre un grupo y otro) se pueda rechazar, en otro caso no hay diferencia significativa entre esos grupos."""

print("Resultados del test de Tukey para 'sexo_jefe':")
print(tukey_test(df, 'ingtrab', 'sexo_jefe'))

"""Primeramente para el sexo del jefe del hogar, la diferencia que hay entre hombres y mujeres con respecto a su ingreso por trabajo es significativa de acuerdo a su p-value, por lo tanto la hipotesis nula se rechaza."""

print("Resultados del test de Tukey para 'educa_jefe':")
print(tukey_test(df, 'ingtrab', 'educa_jefe'))

"""Ahora para la educacion del jefe del hogar, con respecto a su ingreso por trabajo.

- Las personas que tiene como estudio formal el preescolar, no tienen mucha diferencia significativa con el ingreso hasta las personas que ya tienen un estudio profesional completo, las causas pueden ser a que las personas son de una edad adulta con tiempo ya trabajando y que los que tengan estudio profesional sean recien graduados. (Curioso)

- Tampoco las personas que tienen la primaria terminada como las que tienen la secundaria incompleta hay diferencias significativas. Practicamente los dos grupos tienen la misma escolaridad.

- Entre todos los demas grupos si existe una diferencia significativa.
"""

print("Resultados del test de Tukey para 'est_socio':")
print(tukey_test(df, 'ingtrab', 'est_socio'))

"""Para los estratos socioeconomicos tambien existe una diferencia significativa entre los grupos, sobre todo entre los que son de un estrato bajo con los de estrato alto, pero tambien entre los de estrato bajo con los de medio alto."""

print("Resultados del test de Tukey para 'tam_loc':")
print(tukey_test(df, 'ingtrab', 'tam_loc'))

"""En cuestion del tamaño de la localidad si existe diferencia entre los grupos que pertenecen a una localidad con un tamaño en concreto. La prueba nos dice que en localidades de 100,000 habitantes (normalmente grandes ciudades) existe una gran diferencia entre las personas que son de comunidades rurales de alrededor de menos de 2,500 habitantes.

## Correlaciones entre las variables

### Pearson
"""

#Pearson
def pearson_correlation(df, target, numerical_vars):
    results = {}
    for var in numerical_vars:
        corr, p_value = stats.pearsonr(df[target], df[var])
        results[var] = {'correlation': corr, 'p-value': p_value}
    return pd.DataFrame(results).T

numerical_vars = df.select_dtypes(include=float).columns.tolist()
target = 'ingtrab'
pearson_results = pearson_correlation(df, target, numerical_vars)
print("Resultados de la correlación de Pearson:")
print(pearson_results)

"""Las correlaciones nos indican que la varibales trabajo es la que mas se relaciona con la de ingreso por trabajo, aunque tambien el gasto monetario que se hace en los hogares

### Spearman
"""

#Spearman
def spearman_correlation(df, target, numerical_vars):
    results = {}
    for var in numerical_vars:
        corr, p_value = stats.spearmanr(df[target], df[var])
        results[var] = {'correlation': corr, 'p-value': p_value}
    return pd.DataFrame(results).T

spearman_results = spearman_correlation(df, target, numerical_vars)
print("Resultados de la correlación de Spearman:")
print(spearman_results)

"""## Construcción del modelo

### Modelos seleccionados: Regresion lineal multiple, Polinomica, Arbol de Decisión, Bosque aleatorio
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline

"""### División del conjunto de datos"""

X = df.drop(columns=['ingtrab'])
y = df['ingtrab']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)

"""### Regresión lineal"""

linear_model = LinearRegression()
resultados = cross_val_score(linear_model, X_train, y_train, cv=5, scoring='r2').mean()

print("Resultados de la validación cruzada del modelo lineal:")
print(resultados)

"""### Regresion polinomica"""

pipeline = Pipeline([
    ('poly_features', PolynomialFeatures(degree=2)),
    ('linear_regression', LinearRegression())
])

resultados_poly = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2').mean()
print("Resultados de la validación cruzada del modelo polinómico:")
print(resultados_poly)

"""### Árbol de decisión"""

tree = DecisionTreeRegressor(random_state=0)
resultados_tree = cross_val_score(tree, X_train, y_train, cv=5, scoring='r2').mean()

print("Resultados de la validación cruzada del modelo de árbol de decisión:")
print(resultados_tree)

"""### Bosque Aleatorio"""

bosque = RandomForestRegressor(random_state=0)
resultados_bosque = cross_val_score(bosque, X_train, y_train, cv=5, scoring='r2').mean()

print("Resultados de la validación cruzada del modelo de bosque aleatorio:")
print(resultados_bosque)

"""## Modelo final

#### Finalmente la regresión polinomica fue la que tuvo mejor desempeño, aunque por cuestión de recursos tambien el bosque aleatorio se podria tomar en cuenta.

#### Regresión polinomica
"""

pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Resultados del modelo polinómico:")
print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

"""### Ecuación de nuestro modelo de regresión polinomica"""

variables = pipeline.named_steps['poly_features'].get_feature_names_out(X.columns)

# Obtener coeficientes
coefs = pipeline.named_steps['linear_regression'].coef_
intercept = pipeline.named_steps['linear_regression'].intercept_

#Transformar los coeficientes de vuelta al espacio original
coefs = np.exp(coefs) - 1
intercept = np.exp(intercept) - 1

# Armar la ecuación en formato texto
ecuacion = f"y = {intercept:.3f}"
for coef, nombre in zip(coefs, variables):
    sign = " + " if coef >= 0 else " - "
    ecuacion += f"{sign}{abs(coef):.3f}*{nombre}"

print(ecuacion)

"""### Bosque aleatorio"""

bosque.fit(X_train, y_train)
y_pred_bosque = bosque.predict(X_test)

mse_bosque = mean_squared_error(y_test, y_pred_bosque)
r2_bosque = r2_score(y_test, y_pred_bosque)

print("Resultados del modelo de bosque aleatorio:")
print(f"Mean Squared Error: {mse_bosque}")
print(f"R^2 Score: {r2_bosque}")

"""#### Visualización del árbol, de modo que veamos como el bosque funciono"""

#Visualizacion de un arbol del bosque aleatorio
from sklearn.tree import plot_tree

plt.figure(figsize=(20,10))
plot_tree(bosque.estimators_[0], feature_names=X.columns, max_depth=3, filled=True)
plt.show()

"""#### Visualización del bosque"""

#Visualizacion del bosque aleatorio
plt.figure(figsize=(12, 8))
importances = bosque.feature_importances_
indices = np.argsort(importances)[::-1]
plt.bar(range(X_train.shape[1]), importances[indices], align='center')
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.title('Importancia de las Características del Bosque Aleatorio')
plt.xlabel('Características')
plt.ylabel('Importancia')
plt.tight_layout()
plt.show()